{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a63d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6fb97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions=np.array(['palm','peace','thumbs up','thumbs down','rock'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "624b188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d29b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "206cb5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed87d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "489ca332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    rh = np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    lh = np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([lh,rh])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5401207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start\n",
    "\n",
    "import cv2 #open cv\n",
    "import numpy as np\n",
    "import os\n",
    "#from matplotlib import pyplot as plt\n",
    "#import time\n",
    "import mediapipe as mp\n",
    "\n",
    "import webbrowser\n",
    "import pyautogui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "240da26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 5, 64)             48896     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 5, 128)            98816     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 203,525\n",
      "Trainable params: 203,525\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model=load_model('ppt.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4834a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "class SimpleFacerec:\n",
    "    def __init__(self):\n",
    "        self.known_face_encodings = []\n",
    "        self.known_face_names = []\n",
    "\n",
    "        # Resize frame for a faster speed\n",
    "        self.frame_resizing = 0.25\n",
    "\n",
    "    def load_encoding_images(self, images_path):\n",
    "        \"\"\"\n",
    "        Load encoding images from path\n",
    "        :param images_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Load Images\n",
    "        images_path = glob.glob(os.path.join(images_path, \"*.*\"))\n",
    "\n",
    "        print(\"{} encoding images found.\".format(len(images_path)))\n",
    "\n",
    "        # Store image encoding and names\n",
    "        for img_path in images_path:\n",
    "            img = cv2.imread(img_path)\n",
    "            rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Get the filename only from the initial file path.\n",
    "            basename = os.path.basename(img_path)\n",
    "            (filename, ext) = os.path.splitext(basename)\n",
    "            # Get encoding\n",
    "            img_encoding = face_recognition.face_encodings(rgb_img)[0]\n",
    "\n",
    "            # Store file name and file encoding\n",
    "            self.known_face_encodings.append(img_encoding)\n",
    "            self.known_face_names.append(filename)\n",
    "        print(\"Encoding images loaded\")\n",
    "\n",
    "    def detect_known_faces(self, frame):\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=self.frame_resizing, fy=self.frame_resizing)\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(self.known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # # If a match was found in known_face_encodings, just use the first one.\n",
    "            # if True in matches:\n",
    "            #     first_match_index = matches.index(True)\n",
    "            #     name = known_face_names[first_match_index]\n",
    "\n",
    "            # Or instead, use the known face with the smallest distance to the new face\n",
    "            face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = self.known_face_names[best_match_index]\n",
    "            face_names.append(name)\n",
    "\n",
    "        # Convert to numpy array to adjust coordinates with frame resizing quickly\n",
    "        face_locations = np.array(face_locations)\n",
    "        face_locations = face_locations / self.frame_resizing\n",
    "        return face_locations.astype(int), face_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99e7fdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 encoding images found.\n",
      "Encoding images loaded\n"
     ]
    }
   ],
   "source": [
    "sfr = SimpleFacerec()\n",
    "sfr.load_encoding_images(\"images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a96be413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thumbs down\n",
      "thumbs down\n",
      "thumbs down\n",
      "thumbs up\n",
      "palm\n",
      "palm\n",
      "palm\n",
      "palm\n",
      "palm\n",
      "palm\n",
      "palm\n",
      "palm\n",
      "palm\n",
      "peace\n",
      "peace\n",
      "peace\n",
      "peace\n",
      "peace\n",
      "peace\n",
      "peace\n",
      "peace\n",
      "peace\n",
      "peace\n",
      "palm\n",
      "palm\n",
      "palm\n",
      "palm\n",
      "palm\n",
      "palm\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "\n",
    "def reload():\n",
    "    #cv2.waitKey(1000)\n",
    "    cv2.destroyAllWindows()\n",
    "    #cap.release()\n",
    "    time.sleep(2)\n",
    "    startnew()\n",
    "name=''\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "classNames=['palm','peace','rock','thumbs up','thumbs down']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model\n",
    "def startnew():\n",
    "    global sequence,sentence,predictions,threshold,name\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.6,static_image_mode=1) as holistic:\n",
    "        while cap.isOpened():\n",
    "\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "                        # Detect Faces\n",
    "            face_locations, face_names = sfr.detect_known_faces(frame)\n",
    "            for face_loc, name in zip(face_locations, face_names):\n",
    "                y1, x2, y2, x1 = face_loc[0], face_loc[1], face_loc[2], face_loc[3]\n",
    "\n",
    "                #cv2.putText(frame, name,(x1, y1 - 10), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 200), 2)\n",
    "                #cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 200), 4)\n",
    "            #cv2.imshow(\"Gesture detection\", frame)\n",
    "            # Make detectios\n",
    "            if name==\"jayavibhav\":\n",
    "                results = mediapipe_detection(frame, holistic)\n",
    "    #                print(results)\n",
    "\n",
    "                    # Draw landmarks\n",
    "    #                draw_styled_landmarks(frame, results)\n",
    "\n",
    "                    # 2. Prediction logic\n",
    "                keypoints = extract_keypoints(results)\n",
    "                sequence.append(keypoints)\n",
    "                sequence = sequence[-5:]\n",
    "\n",
    "                className=''\n",
    "\n",
    "                if len(sequence) == 5:\n",
    "                    res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                    print(actions[np.argmax(res)])\n",
    "                    predictions.append(np.argmax(res))\n",
    "                    classID = np.argmax(res)\n",
    "                    className = classNames[classID]\n",
    "\n",
    "\n",
    "                    #3. Viz logic\n",
    "                    if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                        if res[np.argmax(res)] > threshold: \n",
    "                            if len(sentence) > 0: \n",
    "                                if actions[np.argmax(res)] != sentence[-1]:\n",
    "                                    sentence.append(actions[np.argmax(res)])\n",
    "                            else:\n",
    "                                sentence.append(actions[np.argmax(res)])\n",
    "                    if len(sentence) > 5: \n",
    "                        sentence = sentence[-5:]\n",
    "\n",
    "                        # Viz probabilities\n",
    "                        #image = prob_viz(res, actions, image, colors)\n",
    "\n",
    "                    #print(className)\n",
    "\n",
    "                    # show the prediction on the frame\n",
    "                    #if className!='thumbs up':\n",
    "                    cv2.putText(frame, className, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (200,20,198), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow('gesture recognition', frame)\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "startnew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b76fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d11b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712f856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
